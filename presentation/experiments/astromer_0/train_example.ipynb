{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94a3199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n"
     ]
    }
   ],
   "source": [
    "cd /home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b16972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.models import get_ASTROMER, build_input\n",
    "from src.data import pretraining_pipeline\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9d0f608",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m----> 3\u001b[0m astromer \u001b[38;5;241m=\u001b[39m  \u001b[43mget_ASTROMER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                         \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mpe_c\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/src/models/zero.py:36\u001b[0m, in \u001b[0;36mget_ASTROMER\u001b[0;34m(num_layers, d_model, num_heads, dff, base, dropout, no_train, maxlen, batch_size, pe_c)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ASTROMER\u001b[39m(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     25\u001b[0m                  d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     26\u001b[0m                  num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m                  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m                  pe_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     34\u001b[0m     placeholder \u001b[38;5;241m=\u001b[39m build_input(maxlen)\n\u001b[0;32m---> 36\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                      \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpe_c\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpe_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_train:\n\u001b[1;32m     46\u001b[0m         encoder\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/home/src/layers/encoder.py:78\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self, num_layers, d_model, num_heads, dff, base, dropout, pe_c, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe \u001b[38;5;241m=\u001b[39m PositionalEncoder(d_model, base\u001b[38;5;241m=\u001b[39mbase, c\u001b[38;5;241m=\u001b[39mpe_c, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosEncoding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_transform \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(d_model)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_layers \u001b[38;5;241m=\u001b[39m [EncoderLayer(d_model, num_heads, dff, dropout, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "File \u001b[0;32m/home/src/layers/encoder.py:78\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe \u001b[38;5;241m=\u001b[39m PositionalEncoder(d_model, base\u001b[38;5;241m=\u001b[39mbase, c\u001b[38;5;241m=\u001b[39mpe_c, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosEncoding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_transform \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(d_model)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_layers \u001b[38;5;241m=\u001b[39m [\u001b[43mEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "File \u001b[0;32m/home/src/layers/encoder.py:22\u001b[0m, in \u001b[0;36mEncoderLayer.__init__\u001b[0;34m(self, d_model, num_heads, dff, rate, use_leak, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_leak \u001b[38;5;241m=\u001b[39m use_leak\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# = ======================== = ======================== =\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha \u001b[38;5;241m=\u001b[39m \u001b[43mHeadAttentionMulti\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn \u001b[38;5;241m=\u001b[39m point_wise_feed_forward_network(d_model, dff)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "File \u001b[0;32m/home/src/layers/attention.py:44\u001b[0m, in \u001b[0;36mHeadAttentionMulti.__init__\u001b[0;34m(self, d_model, num_heads, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model, num_heads, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHeadAttentionMulti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;241m=\u001b[39m d_model\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "window_size = 200\n",
    "\n",
    "astromer =  get_ASTROMER(num_layers=2,\n",
    "                         d_model=16,\n",
    "                         num_heads=2,\n",
    "                         dff=128,\n",
    "                         base=1000,\n",
    "                         dropout=.1,\n",
    "                         maxlen=window_size,\n",
    "                         pe_c=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25af227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(1e-3, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "astromer.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "554c1d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Repeating dataset x1 times\n",
      "[INFO] Repeating dataset x1 times\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/records/alcock/fold_0/alcock_20/'\n",
    "data = dict()\n",
    "for subset in ['train', 'val']:\n",
    "    data[subset] = pretraining_pipeline(os.path.join(data_dir, subset), \n",
    "                                        16, window_size, .2, .1, .1,\n",
    "                                        sampling=True, shuffle=True, repeat=1, num_cls=None,\n",
    "                                        normalize=True, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "faaa7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = astromer.fit(data['train'], epochs=1, validation_data=data['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ca5ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf \n",
    "\n",
    "# min_index = tf.argmin(hist.history['val_loss'])\n",
    "# {\n",
    "#  'train_loss':hist.history['loss'][min_index],\n",
    "# 'train_r2':hist.history['r_square'][min_index],\n",
    "# 'val_loss':hist.history['val_loss'][min_index],\n",
    "# 'val_r2':hist.history['val_r_square'][min_index]   \n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "633884a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c6ab8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.layers.attention import HeadAttentionMulti\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0432b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0 = HeadAttentionMulti(256, 4)\n",
    "layer_1 = HeadAttentionMulti(256, 4, mode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "98542bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones([2, 5, 1], dtype='float32')\n",
    "mask = np.array([[0, 0, 0, 1, 1], [1, 0, 0, 0, 1]], dtype='float32')[..., None]\n",
    "\n",
    "output_0, weights_0 = layer_0(x, mask)\n",
    "output_1, weights_1 = layer_1(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89c1da31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 5, 5), dtype=float32, numpy=\n",
       " array([[[0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]],\n",
       " \n",
       "        [[0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]],\n",
       " \n",
       "        [[0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]],\n",
       " \n",
       "        [[0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 256), dtype=float32, numpy=\n",
       " array([[ 0.01728809,  0.08394676,  0.08468733, ...,  0.13319936,\n",
       "          0.01509266, -0.15529563],\n",
       "        [ 0.01728809,  0.08394676,  0.08468733, ...,  0.13319936,\n",
       "          0.01509266, -0.15529563],\n",
       "        [ 0.01728809,  0.08394676,  0.08468733, ...,  0.13319936,\n",
       "          0.01509266, -0.15529563],\n",
       "        [ 0.0172881 ,  0.08394676,  0.08468734, ...,  0.13319936,\n",
       "          0.01509266, -0.15529564],\n",
       "        [ 0.0172881 ,  0.08394676,  0.08468734, ...,  0.13319936,\n",
       "          0.01509266, -0.15529564]], dtype=float32)>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_0[0], output_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1cb8372f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 256), dtype=float32, numpy=\n",
       "array([[2.559998, 2.559998, 2.559998, ..., 2.559998, 2.559998, 2.559998],\n",
       "       [2.559998, 2.559998, 2.559998, ..., 2.559998, 2.559998, 2.559998],\n",
       "       [2.559998, 2.559998, 2.559998, ..., 2.559998, 2.559998, 2.559998],\n",
       "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
       "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77812a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 1), dtype=float32, numpy=\n",
       "array([[[0.6619955],\n",
       "        [0.6619955],\n",
       "        [0.6619955],\n",
       "        [0.6619955],\n",
       "        [0.6619955]],\n",
       "\n",
       "       [[0.6619955],\n",
       "        [0.6619955],\n",
       "        [0.6619955],\n",
       "        [0.6619955],\n",
       "        [0.6619955]]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=32, value_dim=32)\n",
    "\n",
    "batch_size = 2\n",
    "x = np.ones([batch_size, 5, 1], dtype='float32')\n",
    "mask = np.array([[0, 0, 0, 1, 1], [1, 0, 0, 0, 1]], dtype='float32')\n",
    "mask = tf.tile(mask, [1, 1, ])\n",
    "\n",
    "layer(x, x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99100972",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b29f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def split_heads( x, batch_size, depth, name='qkv'):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"    \n",
    "    x = tf.reshape(x, (batch_size, -1, num_heads, depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3], name=name)\n",
    "    \n",
    "batch_size = 2\n",
    "x = np.ones([batch_size, 5, 1], dtype='float32')\n",
    "mask = np.array([[0, 0, 0, 1, 1], [1, 0, 0, 0, 1]], dtype='float32')[..., None]\n",
    "\n",
    "num_heads = 2\n",
    "head_dim  = 5\n",
    "d_model = num_heads * head_dim\n",
    "\n",
    "initializer = tf.keras.initializers.Constant(-.1)\n",
    "wq = tf.keras.layers.Dense(d_model, kernel_initializer=initializer, use_bias=False, name='WQ')\n",
    "wk = tf.keras.layers.Dense(d_model, kernel_initializer=initializer, use_bias=False, name='WK')\n",
    "wv = tf.keras.layers.Dense(d_model, kernel_initializer=initializer, use_bias=False, name='WV')\n",
    "dense = tf.keras.layers.Dense(d_model,kernel_initializer=initializer, use_bias=False, name='MixerDense')\n",
    "   \n",
    "q = wq(x)  # (batch_size, seq_len, d_model)\n",
    "k = wk(x)  # (batch_size, seq_len, d_model)\n",
    "v = wv(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "q = split_heads(q, batch_size, depth=head_dim, name='Q')  # (batch_size, num_heads, seq_len_q, depth)\n",
    "k = split_heads(k, batch_size, depth=head_dim, name='K')  # (batch_size, num_heads, seq_len_k, depth)\n",
    "v = split_heads(v, batch_size, depth=head_dim, name='V')  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "# scale matmul_qk\n",
    "dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "steps = tf.shape(scaled_attention_logits)[2]\n",
    "mask_rshp = tf.tile(mask, [1,1,steps])\n",
    "mask_rshp += tf.transpose(mask_rshp, [0,2,1])\n",
    "mask_rshp = tf.minimum(1., mask_rshp)\n",
    "mask_rshp = tf.expand_dims(mask_rshp, 1)\n",
    "scaled_attention_logits += (mask_rshp*-1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "086cd950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 5, 5), dtype=float32, numpy=\n",
       "array([[[[ 2.236068e-02,  2.236068e-02,  2.236068e-02, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [ 2.236068e-02,  2.236068e-02,  2.236068e-02, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [ 2.236068e-02,  2.236068e-02,  2.236068e-02, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09]],\n",
       "\n",
       "        [[ 2.236068e-02,  2.236068e-02,  2.236068e-02, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [ 2.236068e-02,  2.236068e-02,  2.236068e-02, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [ 2.236068e-02,  2.236068e-02,  2.236068e-02, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09]]],\n",
       "\n",
       "\n",
       "       [[[-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09,  2.236068e-02,  2.236068e-02,  2.236068e-02,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09,  2.236068e-02,  2.236068e-02,  2.236068e-02,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09,  2.236068e-02,  2.236068e-02,  2.236068e-02,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09]],\n",
       "\n",
       "        [[-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09,  2.236068e-02,  2.236068e-02,  2.236068e-02,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09,  2.236068e-02,  2.236068e-02,  2.236068e-02,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09,  2.236068e-02,  2.236068e-02,  2.236068e-02,\n",
       "          -1.000000e+09],\n",
       "         [-1.000000e+09, -1.000000e+09, -1.000000e+09, -1.000000e+09,\n",
       "          -1.000000e+09]]]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_attention_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ed1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
