{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer\n"
     ]
    }
   ],
   "source": [
    "%cd ../../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 12:55:32.447101: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-19 12:55:33.822073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from src.data.record import deserialize\n",
    "from src.layers.positional import PositionalEmbedding\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 12:55:39.899002: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "path_dataset = './data/records/alcock/fold_0/alcock_20/train'\n",
    "\n",
    "rec_paths = []\n",
    "for folder in os.listdir(path_dataset):\n",
    "    if folder.endswith('.csv'):\n",
    "        continue\n",
    "    for x in os.listdir(os.path.join(path_dataset, folder)):\n",
    "        rec_paths.append(os.path.join(path_dataset, folder, x))\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(rec_paths)    \n",
    "dataset = dataset.map(deserialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([635, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for lc_info in dataset:\n",
    "    lc_data = lc_info['input']\n",
    "    break\n",
    "\n",
    "lc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 3), dtype=float32, numpy=\n",
       "array([[ 4.8917543e+04, -5.0609999e+00,  3.7999999e-02],\n",
       "       [ 4.8918762e+04, -5.0390000e+00,  3.4000002e-02],\n",
       "       [ 4.8919555e+04, -5.0730000e+00,  8.3999999e-02],\n",
       "       [ 4.8920539e+04, -5.1550002e+00,  2.8999999e-02],\n",
       "       [ 4.8927508e+04, -5.3639998e+00,  6.1999999e-02],\n",
       "       [ 4.8928543e+04, -5.0679998e+00,  4.3000001e-02],\n",
       "       [ 4.8929488e+04, -4.9740000e+00,  6.4000003e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_lc_data = lc_data[0:7,:]\n",
    "toy_lc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 1), dtype=float32, numpy=\n",
       "array([[[48917.543],\n",
       "        [48918.76 ],\n",
       "        [48919.555],\n",
       "        [48920.54 ],\n",
       "        [48927.508],\n",
       "        [48928.543],\n",
       "        [48929.49 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_lc_time = toy_lc_data[:,0]\n",
    "toy_lc_flux = toy_lc_data[:,1]\n",
    "\n",
    "toy_lc_time = tf.expand_dims(tf.expand_dims(toy_lc_time, 1), 0)\n",
    "toy_lc_flux = tf.expand_dims(tf.expand_dims(toy_lc_flux, 1), 0)\n",
    "\n",
    "toy_lc_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 6), dtype=float32, numpy=\n",
       "array([[[ 0.19498317,  0.9931323 , -0.2899213 ,  0.31996304,\n",
       "         -0.79138386, -0.7297554 ],\n",
       "        [-0.8534188 ,  0.96417606, -0.40430143,  0.28330478,\n",
       "         -0.78388166, -0.72711056],\n",
       "        [-0.97021437,  0.8680906 , -0.47531015,  0.25910154,\n",
       "         -0.77893037, -0.7253903 ],\n",
       "        [-0.33512622,  0.6748755 , -0.5596383 ,  0.22891594,\n",
       "         -0.77273047, -0.7232442 ],\n",
       "        [ 0.33709115, -0.9942284 , -0.9610362 ,  0.01062681,\n",
       "         -0.7266476 , -0.7078535 ],\n",
       "        [ 0.9816599 , -0.9759721 , -0.98445475, -0.02208604,\n",
       "         -0.7195014 , -0.7055324 ],\n",
       "        [ 0.7293001 , -0.86857057, -0.996654  , -0.05197164,\n",
       "         -0.7128991 , -0.7034098 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model      = 6\n",
    "base         = 1000\n",
    "c            = 1\n",
    "use_mjd      = True\n",
    "pe_trainable = False\n",
    "initializer  = 'pe_astromer' \n",
    "min_period   = 0.01\n",
    "max_period   = 1825  # 5.*365\n",
    "\n",
    "pos_embedding = PositionalEmbedding(d_model, base=1000, c=1, use_mjd=True, pe_trainable=False, \n",
    "\t\t\t  \t                    initializer='pe_astromer', min_period=0.01, max_period=5.*365)\n",
    "\n",
    "pe_t = pos_embedding(toy_lc_time)\n",
    "pe_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import math\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "Initializer = tf.keras.initializers.Initializer\n",
    "\n",
    "def assert_rank(tensor, expected_rank, name=None):\n",
    "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
    "\n",
    "  Args:\n",
    "    tensor: A tf.Tensor to check the rank of.\n",
    "    expected_rank: Python integer or list of integers, expected rank.\n",
    "    name: Optional name of the tensor for the error message.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the expected shape doesn't match the actual shape.\n",
    "  \"\"\"\n",
    "  expected_rank_dict = {}\n",
    "  if isinstance(expected_rank, six.integer_types):\n",
    "    expected_rank_dict[expected_rank] = True\n",
    "  else:\n",
    "    for x in expected_rank:\n",
    "      expected_rank_dict[x] = True\n",
    "\n",
    "  actual_rank = tensor.shape.ndims\n",
    "  if actual_rank not in expected_rank_dict:\n",
    "    raise ValueError(\n",
    "        \"For the tensor `%s`, the actual tensor rank `%d` (shape = %s) is not \"\n",
    "        \"equal to the expected tensor rank `%s`\" %\n",
    "        (name, actual_rank, str(tensor.shape), str(expected_rank)))\n",
    "\n",
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
    "\n",
    "  Args:\n",
    "    tensor: A tf.Tensor object to find the shape of.\n",
    "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
    "      specified and the `tensor` has a different rank, and exception will be\n",
    "      thrown.\n",
    "    name: Optional name of the tensor for the error message.\n",
    "\n",
    "  Returns:\n",
    "    A list of dimensions of the shape of tensor. All static dimensions will\n",
    "    be returned as python integers, and dynamic dimensions will be returned\n",
    "    as tf.Tensor scalars.\n",
    "  \"\"\"\n",
    "  if expected_rank is not None:\n",
    "    assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "  shape = tensor.shape.as_list()\n",
    "\n",
    "  non_static_indexes = []\n",
    "  for (index, dim) in enumerate(shape):\n",
    "    if dim is None:\n",
    "      non_static_indexes.append(index)\n",
    "\n",
    "  if not non_static_indexes:\n",
    "    return shape\n",
    "\n",
    "  dyn_shape = tf.shape(tensor)\n",
    "  for index in non_static_indexes:\n",
    "    shape[index] = dyn_shape[index]\n",
    "  return shape\n",
    "\n",
    "def _relative_position_bucket(relative_position,\n",
    "                              bidirectional=True,\n",
    "                              num_buckets=32,\n",
    "                              max_distance=128):\n",
    "  \"\"\"Translate relative position to a bucket number for relative attention.\n",
    "\n",
    "  The relative position is defined as memory_position - query_position, i.e.\n",
    "  the distance in tokens from the attending position to the attended-to\n",
    "  position.\n",
    "\n",
    "  If `bidirectional=False`, then positive relative positions are invalid.\n",
    "\n",
    "  We use smaller buckets for small absolute relative_position and larger\n",
    "  buckets for larger absolute relative_positions.\n",
    "\n",
    "  All relative positions >=max_distance map to the same bucket.\n",
    "\n",
    "  All relative positions <=-max_distance map to the same bucket.\n",
    "\n",
    "  This should allow for more graceful generalization to longer sequences\n",
    "  than the model has been trained on.\n",
    "\n",
    "  Args:\n",
    "    relative_position: An int32 Tensor\n",
    "    bidirectional: A boolean - whether the attention is bidirectional\n",
    "    num_buckets: An integer\n",
    "    max_distance: An integer\n",
    "\n",
    "  Returns:\n",
    "    A Tensor with the same shape as relative_position, containing int32\n",
    "    values in the range [0, num_buckets)\n",
    "  \"\"\"\n",
    "  ret = 0\n",
    "  n = -relative_position\n",
    "  if bidirectional:\n",
    "    num_buckets //= 2\n",
    "    ret += tf.cast(tf.math.less(n, 0), tf.int32) * num_buckets\n",
    "    n = tf.math.abs(n)\n",
    "    print('num_buckets: {}'.format(num_buckets))\n",
    "    print('ret:\\n{}'.format(ret))\n",
    "  else:\n",
    "    n = tf.math.maximum(n, 0)\n",
    "  \n",
    "  print('n:\\n{}'.format(n))\n",
    "\n",
    "  # now n is in the range [0, inf)\n",
    "  max_exact = num_buckets // 2\n",
    "  is_small = tf.math.less(n, max_exact)\n",
    "  print('max_exact: {}'.format(max_exact))\n",
    "  print('is_small:\\n{}'.format(is_small))\n",
    "\n",
    "  val_if_large = max_exact + tf.dtypes.cast(\n",
    "      tf.math.log(tf.cast(n, tf.float32) / max_exact) /\n",
    "      math.log(max_distance / max_exact) * (num_buckets - max_exact),\n",
    "      tf.int32,\n",
    "  )\n",
    "  print('val_if_large:\\n{}'.format(val_if_large))\n",
    "  val_if_large = tf.math.minimum(val_if_large, num_buckets - 1)\n",
    "  print('val_if_large:\\n{}'.format(val_if_large))\n",
    "  ret += tf.where(is_small, n, val_if_large)\n",
    "  print('ret:\\n{}'.format(ret))\n",
    "  return ret\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Text\")\n",
    "class RelativePositionBias(tf.keras.layers.Layer):\n",
    "  \"\"\"Relative position embedding via per-head bias in T5 style.\n",
    "\n",
    "  Reference implementation in MeshTF:\n",
    "  https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L1000\n",
    "\n",
    "  This layer implements the relative position bias used in \"Exploring the Limits\n",
    "  of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
    "  (https://arxiv.org/abs/1910.10683)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads: int,\n",
    "               relative_attention_num_buckets: int = 32,\n",
    "               relative_attention_max_distance: int = 128,\n",
    "               bidirectional: bool = True,\n",
    "               embeddings_initializer: Optional[Initializer] = None,\n",
    "               **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_heads = num_heads\n",
    "    self.relative_attention_num_buckets = relative_attention_num_buckets\n",
    "    self.bidirectional = bidirectional\n",
    "    self.relative_attention_max_distance = relative_attention_max_distance\n",
    "    if embeddings_initializer:\n",
    "      self._embed_init = embeddings_initializer\n",
    "    else:\n",
    "      self._embed_init = tf.keras.initializers.TruncatedNormal(stddev=1.0)\n",
    "    with tf.name_scope(self.name):\n",
    "      self._relative_attention_bias = self.add_weight(\n",
    "          \"rel_embedding\",\n",
    "          shape=[self.relative_attention_num_buckets, self.num_heads],\n",
    "          initializer=self._embed_init,\n",
    "          dtype=self.dtype,\n",
    "          trainable=True)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        \"num_heads\":\n",
    "            self.num_heads,\n",
    "        \"relative_attention_num_buckets\":\n",
    "            self.relative_attention_num_buckets,\n",
    "        \"relative_attention_max_distance\":\n",
    "            self.relative_attention_max_distance,\n",
    "        \"bidirectional\":\n",
    "            self.bidirectional,\n",
    "        \"embeddings_initializer\":\n",
    "            tf.keras.initializers.serialize(self._embed_init),\n",
    "    }\n",
    "    base_config = super().get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  def call(self, query: tf.Tensor, key: tf.Tensor):\n",
    "    \"\"\"Implements the forward pass.\n",
    "\n",
    "    Args:\n",
    "      query: query input tensor shape [batch, query length, hidden size].\n",
    "      key: key input tensor shape [batch, key length, hidden size].\n",
    "\n",
    "    Returns:\n",
    "      A tensor in shape of [batch, heads, query length, key length].\n",
    "    \"\"\"\n",
    "    print('**************** RELATIVE POSITION BIAS ********************')\n",
    "    batch_size, qlen = get_shape_list(query)[:2]\n",
    "    print('batch_size: {}'.format(batch_size))\n",
    "    print('qlen: {}'.format(qlen))\n",
    "\n",
    "    klen = get_shape_list(key)[1]\n",
    "    print('klen: {}'.format(klen))\n",
    "\n",
    "    context_position = tf.range(qlen)[:, None]\n",
    "    memory_position = tf.range(klen)[None, :]\n",
    "    relative_position = memory_position - context_position\n",
    "    print('context_position: {}'.format(context_position))\n",
    "    print('memory_position: {}'.format(memory_position))\n",
    "    print('relative_position: {}'.format(relative_position))\n",
    "\n",
    "    print('**************** Relative position bucket ********************')\n",
    "\n",
    "    rp_bucket = _relative_position_bucket(\n",
    "        relative_position,\n",
    "        bidirectional=self.bidirectional,\n",
    "        num_buckets=self.relative_attention_num_buckets,\n",
    "        max_distance=self.relative_attention_max_distance)\n",
    "    \n",
    "    print('**************** embedding_lookup ********************')\n",
    "\n",
    "    print('self._relative_attention_bias: ', self._relative_attention_bias)\n",
    "    print('rp_bucket: ', rp_bucket)\n",
    "\n",
    "    values = tf.nn.embedding_lookup(self._relative_attention_bias, rp_bucket)\n",
    "    print('values: ', values)\n",
    "    values = tf.expand_dims(\n",
    "        tf.transpose(values, [2, 0, 1]),\n",
    "        axis=0)  # shape (1, num_heads, qlen, klen)\n",
    "    print('values: ', values)\n",
    "    values = tf.tile(values, [batch_size, 1, 1, 1])\n",
    "    print('values: ', values)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 6), dtype=float32, numpy=\n",
       "array([[[ 4.2836976 ,  3.1413937 ,  1.1413734 , -0.9834503 ,\n",
       "          1.6195555 , -0.44428775],\n",
       "        [ 4.2650766 ,  3.1277382 ,  1.1364119 , -0.97917527,\n",
       "          1.6125154 , -0.44235647],\n",
       "        [ 4.293854  ,  3.1488423 ,  1.1440797 , -0.98578215,\n",
       "          1.6233957 , -0.4453412 ],\n",
       "        [ 4.3632603 ,  3.1997402 ,  1.1625726 , -1.0017164 ,\n",
       "          1.6496363 , -0.4525397 ],\n",
       "        [ 4.5401607 ,  3.3294675 ,  1.2097069 , -1.0423291 ,\n",
       "          1.7165177 , -0.47088706],\n",
       "        [ 4.2896223 ,  3.1457386 ,  1.142952  , -0.98481053,\n",
       "          1.6217955 , -0.44490224],\n",
       "        [ 4.2100596 ,  3.0873923 ,  1.1217529 , -0.9665445 ,\n",
       "          1.591715  , -0.43665034]]], dtype=float32)>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq = tf.keras.layers.Dense(d_model, name='WQ')\n",
    "wk = tf.keras.layers.Dense(d_model, name='WK')\n",
    "\n",
    "query = wq(toy_lc_flux)\n",
    "key = wk(toy_lc_flux)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** RELATIVE POSITION BIAS ********************\n",
      "batch_size: 1\n",
      "qlen: 7\n",
      "klen: 7\n",
      "context_position: [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]]\n",
      "memory_position: [[0 1 2 3 4 5 6]]\n",
      "relative_position: [[ 0  1  2  3  4  5  6]\n",
      " [-1  0  1  2  3  4  5]\n",
      " [-2 -1  0  1  2  3  4]\n",
      " [-3 -2 -1  0  1  2  3]\n",
      " [-4 -3 -2 -1  0  1  2]\n",
      " [-5 -4 -3 -2 -1  0  1]\n",
      " [-6 -5 -4 -3 -2 -1  0]]\n",
      "**************** Relative position bucket ********************\n",
      "num_buckets: 4\n",
      "ret:\n",
      "[[0 4 4 4 4 4 4]\n",
      " [0 0 4 4 4 4 4]\n",
      " [0 0 0 4 4 4 4]\n",
      " [0 0 0 0 4 4 4]\n",
      " [0 0 0 0 0 4 4]\n",
      " [0 0 0 0 0 0 4]\n",
      " [0 0 0 0 0 0 0]]\n",
      "n:\n",
      "[[0 1 2 3 4 5 6]\n",
      " [1 0 1 2 3 4 5]\n",
      " [2 1 0 1 2 3 4]\n",
      " [3 2 1 0 1 2 3]\n",
      " [4 3 2 1 0 1 2]\n",
      " [5 4 3 2 1 0 1]\n",
      " [6 5 4 3 2 1 0]]\n",
      "max_exact: 2\n",
      "is_small:\n",
      "[[ True  True False False False False False]\n",
      " [ True  True  True False False False False]\n",
      " [False  True  True  True False False False]\n",
      " [False False  True  True  True False False]\n",
      " [False False False  True  True  True False]\n",
      " [False False False False  True  True  True]\n",
      " [False False False False False  True  True]]\n",
      "val_if_large:\n",
      "[[-2147483646           4           2           1           0           0\n",
      "           -1]\n",
      " [          4 -2147483646           4           2           1           0\n",
      "            0]\n",
      " [          2           4 -2147483646           4           2           1\n",
      "            0]\n",
      " [          1           2           4 -2147483646           4           2\n",
      "            1]\n",
      " [          0           1           2           4 -2147483646           4\n",
      "            2]\n",
      " [          0           0           1           2           4 -2147483646\n",
      "            4]\n",
      " [         -1           0           0           1           2           4\n",
      "  -2147483646]]\n",
      "val_if_large:\n",
      "[[-2147483646           3           2           1           0           0\n",
      "           -1]\n",
      " [          3 -2147483646           3           2           1           0\n",
      "            0]\n",
      " [          2           3 -2147483646           3           2           1\n",
      "            0]\n",
      " [          1           2           3 -2147483646           3           2\n",
      "            1]\n",
      " [          0           1           2           3 -2147483646           3\n",
      "            2]\n",
      " [          0           0           1           2           3 -2147483646\n",
      "            3]\n",
      " [         -1           0           0           1           2           3\n",
      "  -2147483646]]\n",
      "ret:\n",
      "[[ 0  5  6  5  4  4  3]\n",
      " [ 1  0  5  6  5  4  4]\n",
      " [ 2  1  0  5  6  5  4]\n",
      " [ 1  2  1  0  5  6  5]\n",
      " [ 0  1  2  1  0  5  6]\n",
      " [ 0  0  1  2  1  0  5]\n",
      " [-1  0  0  1  2  1  0]]\n",
      "**************** embedding_lookup ********************\n",
      "self._relative_attention_bias:  <tf.Variable 'relative_position_bias_9/rel_embedding:0' shape=(8, 6) dtype=float32, numpy=\n",
      "array([[ 0.735682  , -0.17359392, -0.497484  , -1.4142381 , -0.04317945,\n",
      "         0.33456495],\n",
      "       [ 0.78576475,  0.38492027,  0.83819455, -0.3768038 , -0.34997204,\n",
      "         0.9979595 ],\n",
      "       [-0.15020491, -0.65538865,  0.31754538,  1.1988682 ,  1.7163295 ,\n",
      "         1.7764916 ],\n",
      "       [ 0.97997594, -0.03028708,  1.4908278 ,  1.8952681 ,  0.11857565,\n",
      "        -0.22168362],\n",
      "       [ 1.5018709 , -0.01362551, -0.41437584, -1.1240162 , -0.09922825,\n",
      "        -0.29101783],\n",
      "       [ 0.09603965,  0.17271025, -0.42766908,  0.02382101, -0.78635335,\n",
      "        -0.5844859 ],\n",
      "       [-0.7199312 ,  1.8318716 , -0.49056566, -0.12911248,  0.02487486,\n",
      "        -0.59437   ],\n",
      "       [-1.5580338 , -1.5036125 , -0.6080018 ,  0.7689526 , -0.00633646,\n",
      "        -0.4685013 ]], dtype=float32)>\n",
      "rp_bucket:  tf.Tensor(\n",
      "[[ 0  5  6  5  4  4  3]\n",
      " [ 1  0  5  6  5  4  4]\n",
      " [ 2  1  0  5  6  5  4]\n",
      " [ 1  2  1  0  5  6  5]\n",
      " [ 0  1  2  1  0  5  6]\n",
      " [ 0  0  1  2  1  0  5]\n",
      " [-1  0  0  1  2  1  0]], shape=(7, 7), dtype=int32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'relative_position_bias_9' (type RelativePositionBias).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[6,0] = -1 is not in [0, 8) [Op:ResourceGather] name: \n\nCall arguments received by layer 'relative_position_bias_9' (type RelativePositionBias):\n  • query=tf.Tensor(shape=(1, 7, 6), dtype=float32)\n  • key=tf.Tensor(shape=(1, 7, 6), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m relative_attention_max_distance \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m rel_pos_bias \u001b[39m=\u001b[39m RelativePositionBias(num_heads, relative_attention_num_buckets,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                                     bidirectional, relative_attention_max_distance)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m values \u001b[39m=\u001b[39m rel_pos_bias(query, key)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m values\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32m/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=216'>217</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mself._relative_attention_bias: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_relative_attention_bias)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=217'>218</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mrp_bucket: \u001b[39m\u001b[39m'\u001b[39m, rp_bucket)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=219'>220</a>\u001b[0m values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49membedding_lookup(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_relative_attention_bias, rp_bucket)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=220'>221</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mvalues: \u001b[39m\u001b[39m'\u001b[39m, values)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=221'>222</a>\u001b[0m values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=222'>223</a>\u001b[0m     tf\u001b[39m.\u001b[39mtranspose(values, [\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=223'>224</a>\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# shape (1, num_heads, qlen, klen)\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'relative_position_bias_9' (type RelativePositionBias).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[6,0] = -1 is not in [0, 8) [Op:ResourceGather] name: \n\nCall arguments received by layer 'relative_position_bias_9' (type RelativePositionBias):\n  • query=tf.Tensor(shape=(1, 7, 6), dtype=float32)\n  • key=tf.Tensor(shape=(1, 7, 6), dtype=float32)"
     ]
    }
   ],
   "source": [
    "num_heads = d_model\n",
    "relative_attention_num_buckets = 4*2\n",
    "bidirectional = True\n",
    "relative_attention_max_distance = 4\n",
    "\n",
    "rel_pos_bias = RelativePositionBias(num_heads, relative_attention_num_buckets,\n",
    "                                    bidirectional, relative_attention_max_distance)\n",
    "\n",
    "values = rel_pos_bias(query, key)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 7), dtype=float32, numpy=\n",
       "array([[[  0.        ,  -1.21875   ,  -2.0117188 ,  -2.9960938 ,\n",
       "          -9.964844  , -11.        , -11.9453125 ],\n",
       "        [  1.21875   ,   0.        ,  -0.79296875,  -1.7773438 ,\n",
       "          -8.746094  ,  -9.78125   , -10.7265625 ],\n",
       "        [  2.0117188 ,   0.79296875,   0.        ,  -0.984375  ,\n",
       "          -7.953125  ,  -8.988281  ,  -9.933594  ],\n",
       "        [  2.9960938 ,   1.7773438 ,   0.984375  ,   0.        ,\n",
       "          -6.96875   ,  -8.003906  ,  -8.949219  ],\n",
       "        [  9.964844  ,   8.746094  ,   7.953125  ,   6.96875   ,\n",
       "           0.        ,  -1.0351562 ,  -1.9804688 ],\n",
       "        [ 11.        ,   9.78125   ,   8.988281  ,   8.003906  ,\n",
       "           1.0351562 ,   0.        ,  -0.9453125 ],\n",
       "        [ 11.9453125 ,  10.7265625 ,   9.933594  ,   8.949219  ,\n",
       "           1.9804688 ,   0.9453125 ,   0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_lc_time - tf.transpose(toy_lc_time, perm=[0,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debbuging pos rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "qlen: 7\n"
     ]
    }
   ],
   "source": [
    "batch_size, qlen = get_shape_list(query)[:2]\n",
    "print('batch_size: {}'.format(batch_size))\n",
    "print('qlen: {}'.format(qlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klen: 7\n"
     ]
    }
   ],
   "source": [
    "klen = get_shape_list(key)[1]\n",
    "print('klen: {}'.format(klen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 7), dtype=float32, numpy=\n",
       "array([[[48917.543, 48918.76 , 48919.555, 48920.54 , 48927.508,\n",
       "         48928.543, 48929.49 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]], dtype=int32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_position = tf.range(qlen)[:, None]\n",
    "context_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 1, 2, 3, 4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_position = tf.range(klen)[None, :]\n",
    "memory_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "       [-1,  0,  1,  2,  3,  4,  5],\n",
       "       [-2, -1,  0,  1,  2,  3,  4],\n",
       "       [-3, -2, -1,  0,  1,  2,  3],\n",
       "       [-4, -3, -2, -1,  0,  1,  2],\n",
       "       [-5, -4, -3, -2, -1,  0,  1],\n",
       "       [-6, -5, -4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_position = memory_position - context_position\n",
    "relative_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional = True\n",
    "relative_attention_num_buckets = num_buckets = 4\n",
    "relative_attention_max_distance = max_distance = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_buckets: 2\n",
      "ret:\n",
      "[[0 2 2 2 2 2 2]\n",
      " [0 0 2 2 2 2 2]\n",
      " [0 0 0 2 2 2 2]\n",
      " [0 0 0 0 2 2 2]\n",
      " [0 0 0 0 0 2 2]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 0]]\n",
      "n:\n",
      "[[0 1 2 3 4 5 6]\n",
      " [1 0 1 2 3 4 5]\n",
      " [2 1 0 1 2 3 4]\n",
      " [3 2 1 0 1 2 3]\n",
      " [4 3 2 1 0 1 2]\n",
      " [5 4 3 2 1 0 1]\n",
      " [6 5 4 3 2 1 0]]\n",
      "max_exact: 1\n",
      "is_small:\n",
      "[[ True False False False False False False]\n",
      " [False  True False False False False False]\n",
      " [False False  True False False False False]\n",
      " [False False False  True False False False]\n",
      " [False False False False  True False False]\n",
      " [False False False False False  True False]\n",
      " [False False False False False False  True]]\n",
      "val_if_large:\n",
      "[[-2147483647           1           1           1           1           2\n",
      "            2]\n",
      " [          1 -2147483647           1           1           1           1\n",
      "            2]\n",
      " [          1           1 -2147483647           1           1           1\n",
      "            1]\n",
      " [          1           1           1 -2147483647           1           1\n",
      "            1]\n",
      " [          1           1           1           1 -2147483647           1\n",
      "            1]\n",
      " [          2           1           1           1           1 -2147483647\n",
      "            1]\n",
      " [          2           2           1           1           1           1\n",
      "  -2147483647]]\n",
      "val_if_large:\n",
      "[[-2147483647           1           1           1           1           1\n",
      "            1]\n",
      " [          1 -2147483647           1           1           1           1\n",
      "            1]\n",
      " [          1           1 -2147483647           1           1           1\n",
      "            1]\n",
      " [          1           1           1 -2147483647           1           1\n",
      "            1]\n",
      " [          1           1           1           1 -2147483647           1\n",
      "            1]\n",
      " [          1           1           1           1           1 -2147483647\n",
      "            1]\n",
      " [          1           1           1           1           1           1\n",
      "  -2147483647]]\n",
      "ret:\n",
      "[[0 3 3 3 3 3 3]\n",
      " [1 0 3 3 3 3 3]\n",
      " [1 1 0 3 3 3 3]\n",
      " [1 1 1 0 3 3 3]\n",
      " [1 1 1 1 0 3 3]\n",
      " [1 1 1 1 1 0 3]\n",
      " [1 1 1 1 1 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[0, 3, 3, 3, 3, 3, 3],\n",
       "       [1, 0, 3, 3, 3, 3, 3],\n",
       "       [1, 1, 0, 3, 3, 3, 3],\n",
       "       [1, 1, 1, 0, 3, 3, 3],\n",
       "       [1, 1, 1, 1, 0, 3, 3],\n",
       "       [1, 1, 1, 1, 1, 0, 3],\n",
       "       [1, 1, 1, 1, 1, 1, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_bucket = _relative_position_bucket(\n",
    "    relative_position,\n",
    "    bidirectional=bidirectional,\n",
    "    num_buckets=relative_attention_num_buckets,\n",
    "    max_distance=relative_attention_max_distance)\n",
    "\n",
    "rp_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0, -1, -2, -3, -4, -5, -6],\n",
       "       [ 1,  0, -1, -2, -3, -4, -5],\n",
       "       [ 2,  1,  0, -1, -2, -3, -4],\n",
       "       [ 3,  2,  1,  0, -1, -2, -3],\n",
       "       [ 4,  3,  2,  1,  0, -1, -2],\n",
       "       [ 5,  4,  3,  2,  1,  0, -1],\n",
       "       [ 6,  5,  4,  3,  2,  1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = 0\n",
    "n = -relative_position\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_buckets: 2\n",
      "ret:\n",
      "[[0 2 2 2 2 2 2]\n",
      " [0 0 2 2 2 2 2]\n",
      " [0 0 0 2 2 2 2]\n",
      " [0 0 0 0 2 2 2]\n",
      " [0 0 0 0 0 2 2]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 0]]\n",
      "n:\n",
      "[[0 1 2 3 4 5 6]\n",
      " [1 0 1 2 3 4 5]\n",
      " [2 1 0 1 2 3 4]\n",
      " [3 2 1 0 1 2 3]\n",
      " [4 3 2 1 0 1 2]\n",
      " [5 4 3 2 1 0 1]\n",
      " [6 5 4 3 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "if bidirectional:\n",
    "    num_buckets //= 2\n",
    "    ret += tf.cast(tf.math.less(n, 0), tf.int32) * num_buckets\n",
    "    n = tf.math.abs(n)\n",
    "    print('num_buckets: {}'.format(num_buckets))\n",
    "    print('ret:\\n{}'.format(ret))\n",
    "else:\n",
    "    n = tf.math.maximum(n, 0)\n",
    "    \n",
    "print('n:\\n{}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now n is in the range [0, inf)\n",
    "max_exact = num_buckets // 2\n",
    "max_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=bool, numpy=\n",
       "array([[ True, False, False, False, False, False, False],\n",
       "       [False,  True, False, False, False, False, False],\n",
       "       [False, False,  True, False, False, False, False],\n",
       "       [False, False, False,  True, False, False, False],\n",
       "       [False, False, False, False,  True, False, False],\n",
       "       [False, False, False, False, False,  True, False],\n",
       "       [False, False, False, False, False, False,  True]])>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_small = tf.math.less(n, max_exact)\n",
    "is_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[-2147483647,           1,           2,           2,           3,\n",
       "                  3,           3],\n",
       "       [          1, -2147483647,           1,           2,           2,\n",
       "                  3,           3],\n",
       "       [          2,           1, -2147483647,           1,           2,\n",
       "                  2,           3],\n",
       "       [          2,           2,           1, -2147483647,           1,\n",
       "                  2,           2],\n",
       "       [          3,           2,           2,           1, -2147483647,\n",
       "                  1,           2],\n",
       "       [          3,           3,           2,           2,           1,\n",
       "        -2147483647,           1],\n",
       "       [          3,           3,           3,           2,           2,\n",
       "                  1, -2147483647]], dtype=int32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_if_large = max_exact + tf.dtypes.cast(\n",
    "    tf.math.log(tf.cast(n, tf.float32) / max_exact) /\n",
    "    math.log(max_distance / max_exact) * (num_buckets - max_exact),\n",
    "    tf.int32,\n",
    ")\n",
    "\n",
    "val_if_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[-2147483647,           1,           2,           2,           2,\n",
       "                  2,           2],\n",
       "       [          1, -2147483647,           1,           2,           2,\n",
       "                  2,           2],\n",
       "       [          2,           1, -2147483647,           1,           2,\n",
       "                  2,           2],\n",
       "       [          2,           2,           1, -2147483647,           1,\n",
       "                  2,           2],\n",
       "       [          2,           2,           2,           1, -2147483647,\n",
       "                  1,           2],\n",
       "       [          2,           2,           2,           2,           1,\n",
       "        -2147483647,           1],\n",
       "       [          2,           2,           2,           2,           2,\n",
       "                  1, -2147483647]], dtype=int32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_if_large = tf.math.minimum(val_if_large, num_buckets - 1)\n",
    "val_if_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[0, 4, 5, 5, 5, 5, 5],\n",
       "       [1, 0, 4, 5, 5, 5, 5],\n",
       "       [2, 1, 0, 4, 5, 5, 5],\n",
       "       [2, 2, 1, 0, 4, 5, 5],\n",
       "       [2, 2, 2, 1, 0, 4, 5],\n",
       "       [2, 2, 2, 2, 1, 0, 4],\n",
       "       [2, 2, 2, 2, 2, 1, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret += tf.where(is_small, n, val_if_large)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_embed_init = None\n",
    "\n",
    "self._relative_attention_bias = self.add_weight(\n",
    "    \"rel_embedding\",\n",
    "    shape=[self.relative_attention_num_buckets, self.num_heads],\n",
    "    initializer=self._embed_init,\n",
    "    dtype=self.dtype,\n",
    "    trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = tf.nn.embedding_lookup(_relative_attention_bias, rp_bucket)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = tf.expand_dims(\n",
    "    tf.transpose(values, [2, 0, 1]),\n",
    "    axis=0)  # shape (1, num_heads, qlen, klen)\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = tf.tile(values, [batch_size, 1, 1, 1])\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "       [-1,  0,  1,  2,  3,  4,  5],\n",
       "       [-2, -1,  0,  1,  2,  3,  4],\n",
       "       [-3, -2, -1,  0,  1,  2,  3],\n",
       "       [-4, -3, -2, -1,  0,  1,  2],\n",
       "       [-5, -4, -3, -2, -1,  0,  1],\n",
       "       [-6, -5, -4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0,  1,  1,  1,  1,  1,  1],\n",
       "       [-1,  0,  1,  1,  1,  1,  1],\n",
       "       [-1, -1,  0,  1,  1,  1,  1],\n",
       "       [-1, -1, -1,  0,  1,  1,  1],\n",
       "       [-1, -1, -1, -1,  0,  1,  1],\n",
       "       [-1, -1, -1, -1, -1,  0,  1],\n",
       "       [-1, -1, -1, -1, -1, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.sign(relative_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "qlen: 7\n",
      "klen: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "       [-1,  0,  1,  2,  3,  4,  5],\n",
       "       [-2, -1,  0,  1,  2,  3,  4],\n",
       "       [-3, -2, -1,  0,  1,  2,  3],\n",
       "       [-4, -3, -2, -1,  0,  1,  2],\n",
       "       [-5, -4, -3, -2, -1,  0,  1],\n",
       "       [-6, -5, -4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, qlen = get_shape_list(query)[:2]\n",
    "print('batch_size: {}'.format(batch_size))\n",
    "print('qlen: {}'.format(qlen))\n",
    "\n",
    "klen = get_shape_list(key)[1]\n",
    "print('klen: {}'.format(klen))\n",
    "\n",
    "context_position = tf.range(qlen)[:, None]\n",
    "memory_position = tf.range(klen)[None, :]\n",
    "relative_position = memory_position - context_position\n",
    "relative_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4,  5,  5],\n",
       "       [-1,  0,  1,  2,  3,  4,  5],\n",
       "       [-2, -1,  0,  1,  2,  3,  4],\n",
       "       [-3, -2, -1,  0,  1,  2,  3],\n",
       "       [-4, -3, -2, -1,  0,  1,  2],\n",
       "       [-5, -4, -3, -2, -1,  0,  1],\n",
       "       [-5, -5, -4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_attention_max_distance = 4\n",
    "relative_attention_num_buckets = 5\n",
    "\n",
    "def make_log_bucket_position(relative_pos, bucket_size, max_position):\n",
    "    sign = tf.math.sign(relative_pos)\n",
    "    mid = bucket_size // 2\n",
    "    abs_pos = tf.where((relative_pos < mid) & (relative_pos > -mid), mid - 1, tf.math.abs(relative_pos))\n",
    "    log_pos = (tf.math.ceil(tf.cast(tf.math.log(abs_pos / mid), tf.float32) / \n",
    "                            tf.math.log((max_position - 1) / mid) * (mid - 1)) \n",
    "                            + mid )\n",
    "    bucket_pos = tf.cast(tf.where(abs_pos <= mid, \n",
    "                                  tf.cast(relative_pos, tf.float32), \n",
    "                                  log_pos * tf.cast(sign, tf.float32)), tf.int32)\n",
    "    return bucket_pos\n",
    "\n",
    "bucket_pos = make_log_bucket_position(relative_pos=relative_position, \n",
    "                                      bucket_size=relative_attention_num_buckets, \n",
    "                                      max_position=relative_attention_max_distance)\n",
    "\n",
    "bucket_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  5,  6,  6],\n",
       "       [-1,  0,  1,  2,  3,  5,  6],\n",
       "       [-2, -1,  0,  1,  2,  3,  5],\n",
       "       [-3, -2, -1,  0,  1,  2,  3],\n",
       "       [-5, -3, -2, -1,  0,  1,  2],\n",
       "       [-6, -5, -3, -2, -1,  0,  1],\n",
       "       [-6, -6, -5, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_attention_max_distance = 6\n",
    "relative_attention_num_buckets = relative_attention_max_distance\n",
    "\n",
    "def make_log_bucket_position(relative_pos, bucket_size, max_position):\n",
    "    sign = tf.math.sign(relative_pos)\n",
    "    mid = bucket_size // 2\n",
    "    abs_pos = tf.where((relative_pos < mid) & (relative_pos > -mid), mid - 1, tf.math.abs(relative_pos))\n",
    "    log_pos = (tf.math.ceil(tf.cast(tf.math.log(abs_pos / mid), tf.float32) / \n",
    "                            tf.math.log((max_position - 1) / mid) * (mid - 1)) \n",
    "                            + mid )\n",
    "    bucket_pos = tf.cast(tf.where(abs_pos <= mid, \n",
    "                                  tf.cast(relative_pos, tf.float32), \n",
    "                                  log_pos * tf.cast(sign, tf.float32)), tf.int32)\n",
    "    return bucket_pos\n",
    "\n",
    "bucket_pos = make_log_bucket_position(relative_pos=relative_position, \n",
    "                                      bucket_size=relative_attention_num_buckets, \n",
    "                                      max_position=relative_attention_max_distance)\n",
    "\n",
    "bucket_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 7), dtype=int32, numpy=\n",
       "array([[[ 0,  1,  2,  3,  4,  5,  5],\n",
       "        [-1,  0,  1,  2,  3,  4,  5],\n",
       "        [-2, -1,  0,  1,  2,  3,  4],\n",
       "        [-3, -2, -1,  0,  1,  2,  3],\n",
       "        [-4, -3, -2, -1,  0,  1,  2],\n",
       "        [-5, -4, -3, -2, -1,  0,  1],\n",
       "        [-5, -5, -4, -3, -2, -1,  0]]], dtype=int32)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_ids = bucket_pos[:qlen, :]\n",
    "rel_pos_ids = tf.expand_dims(rel_pos_ids, axis=0)\n",
    "rel_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_span = relative_attention_max_distance\n",
    "rel_embeddings = tf.expand_dims(\n",
    "    rel_embeddings[relative_attention_max_distance - att_span : relative_attention_max_distance + att_span, :], 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_pos = build_relative_position(q, key_layer.size(-2), bucket_size = self.position_buckets, \\\n",
    "                                       max_position = self.max_relative_positions, device=query_layer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "def shape_list(tensor: Union[tf.Tensor, np.ndarray]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Deal with dynamic shape in tensorflow cleanly.\n",
    "\n",
    "    Args:\n",
    "        tensor (`tf.Tensor` or `np.ndarray`): The tensor we want the shape of.\n",
    "\n",
    "    Returns:\n",
    "        `List[int]`: The shape of the tensor as a list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        return list(tensor.shape)\n",
    "\n",
    "    dynamic = tf.shape(tensor)\n",
    "\n",
    "    if tensor.shape == tf.TensorShape(None):\n",
    "        return dynamic\n",
    "\n",
    "    static = tensor.shape.as_list()\n",
    "\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 x 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_buckets, max_relative_pos, bidirectional, **kwargs):\n",
    "        super(RelativePositionBias, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_relative_pos = max_relative_pos\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.rel_embeddings = self.add_weight(shape=(self.num_buckets, self.d_model),\n",
    "                                             #initializer=self.init_freq,\n",
    "                                             trainable=True,\n",
    "                                             name='rel_emb')\n",
    "\n",
    "\n",
    "    def call(self, q, k, log_bucket=True):\n",
    "        batch_size, qlen = get_shape_list(q)[:2] \n",
    "        klen = get_shape_list(k)[1]\n",
    "        relative_pos = self.get_rel_pos(qlen, klen)\n",
    "        if log_bucket:\n",
    "            relative_pos = self.make_log_bucket_position(relative_pos)\n",
    "        relative_pos = relative_pos[:qlen, :]\n",
    "        relative_pos = tf.expand_dims(relative_pos, axis=0)\n",
    "    \n",
    "        print('relative_pos:', relative_pos)\n",
    "        \n",
    "        shape_list_pos = shape_list(relative_pos)\n",
    "        print('shape_list_pos:', shape_list_pos)\n",
    "\n",
    "        if len(shape_list_pos) == 2:\n",
    "            relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)\n",
    "        elif len(shape_list_pos) == 3:\n",
    "            relative_pos = tf.expand_dims(relative_pos, 1)\n",
    "        # bsz x height x query x key\n",
    "        elif len(shape_list_pos) != 4:\n",
    "            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n",
    "\n",
    "        print('relative_pos:', relative_pos)\n",
    "\n",
    "        att_span = self.max_relative_pos\n",
    "        print('1. self.rel_embeddings:', self.rel_embeddings)\n",
    "        \n",
    "        self.rel_embeddings = tf.expand_dims(\n",
    "            self.rel_embeddings[self.max_relative_pos - att_span : self.max_relative_pos + att_span, :], 0\n",
    "        )\n",
    "\n",
    "        print('2. self.rel_embeddings:', self.rel_embeddings)\n",
    "\n",
    "    def get_rel_pos(self, qlen, klen):\n",
    "        context_position = tf.range(qlen)[:, None]\n",
    "        memory_position = tf.range(klen)[None, :]\n",
    "        relative_pos = memory_position - context_position\n",
    "        return relative_pos\n",
    "\n",
    "\n",
    "    def make_log_bucket_position(self, relative_pos):\n",
    "        sign = tf.math.sign(relative_pos)\n",
    "        mid = self.num_buckets // 2\n",
    "        abs_pos = tf.where((relative_pos < mid) & (relative_pos > -mid), mid - 1, tf.math.abs(relative_pos))\n",
    "        log_pos = (\n",
    "            tf.math.ceil(\n",
    "                tf.cast(tf.math.log(abs_pos / mid), tf.float32) / tf.math.log((self.max_relative_pos - 1) / mid) * (mid - 1)\n",
    "            )\n",
    "            + mid\n",
    "        )\n",
    "        bucket_pos = tf.cast(\n",
    "            tf.where(abs_pos <= mid, tf.cast(relative_pos, tf.float32), log_pos * tf.cast(sign, tf.float32)), tf.int32\n",
    "        )\n",
    "        return bucket_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 6), dtype=float32, numpy=\n",
       "array([[[-0.4049514 , -0.54453087,  4.6461005 , -1.4279401 ,\n",
       "         -4.5876546 , -3.1889935 ],\n",
       "        [-0.4031911 , -0.54216385,  4.6259046 , -1.421733  ,\n",
       "         -4.5677123 , -3.175131  ],\n",
       "        [-0.40591156, -0.545822  ,  4.657117  , -1.4313259 ,\n",
       "         -4.598532  , -3.196555  ],\n",
       "        [-0.41247275, -0.5546447 ,  4.7323947 , -1.4544619 ,\n",
       "         -4.672863  , -3.248224  ],\n",
       "        [-0.42919567, -0.5771317 ,  4.9242606 , -1.5134304 ,\n",
       "         -4.8623157 , -3.3799171 ],\n",
       "        [-0.4055115 , -0.54528403,  4.652527  , -1.4299152 ,\n",
       "         -4.594     , -3.1934042 ],\n",
       "        [-0.39799017, -0.53517026,  4.566233  , -1.4033935 ,\n",
       "         -4.5087914 , -3.1341739 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq = tf.keras.layers.Dense(d_model, name='WQ')\n",
    "wk = tf.keras.layers.Dense(d_model, name='WK')\n",
    "\n",
    "query = wq(toy_lc_flux)\n",
    "key = wk(toy_lc_flux)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_pos: tf.Tensor(\n",
      "[[[ 0  1  2  3  4  5  6]\n",
      "  [-1  0  1  2  3  4  5]\n",
      "  [-2 -1  0  1  2  3  4]\n",
      "  [-3 -2 -1  0  1  2  3]\n",
      "  [-4 -3 -2 -1  0  1  2]\n",
      "  [-5 -4 -3 -2 -1  0  1]\n",
      "  [-6 -5 -4 -3 -2 -1  0]]], shape=(1, 7, 7), dtype=int32)\n",
      "shape_list_pos: [1, 7, 7]\n",
      "relative_pos: tf.Tensor(\n",
      "[[[[ 0  1  2  3  4  5  6]\n",
      "   [-1  0  1  2  3  4  5]\n",
      "   [-2 -1  0  1  2  3  4]\n",
      "   [-3 -2 -1  0  1  2  3]\n",
      "   [-4 -3 -2 -1  0  1  2]\n",
      "   [-5 -4 -3 -2 -1  0  1]\n",
      "   [-6 -5 -4 -3 -2 -1  0]]]], shape=(1, 1, 7, 7), dtype=int32)\n",
      "1. self.rel_embeddings: <tf.Variable 'relative_position_bias_10/rel_emb:0' shape=(5, 6) dtype=float32, numpy=\n",
      "array([[ 0.04249322, -0.02939612,  0.06477737, -0.50271374, -0.02651012,\n",
      "        -0.6698197 ],\n",
      "       [-0.00114155, -0.5151731 , -0.70630115,  0.11261612,  0.6107096 ,\n",
      "        -0.41813314],\n",
      "       [ 0.12850839, -0.55583274,  0.30220467,  0.23526502, -0.17055827,\n",
      "        -0.5858531 ],\n",
      "       [-0.22770166,  0.47143584, -0.55156696, -0.24672067,  0.1727817 ,\n",
      "         0.68841785],\n",
      "       [-0.73660463,  0.04830807,  0.26371783,  0.11195582,  0.6287971 ,\n",
      "        -0.6293106 ]], dtype=float32)>\n",
      "2. self.rel_embeddings: tf.Tensor(\n",
      "[[[ 0.04249322 -0.02939612  0.06477737 -0.50271374 -0.02651012\n",
      "   -0.6698197 ]\n",
      "  [-0.00114155 -0.5151731  -0.70630115  0.11261612  0.6107096\n",
      "   -0.41813314]\n",
      "  [ 0.12850839 -0.55583274  0.30220467  0.23526502 -0.17055827\n",
      "   -0.5858531 ]\n",
      "  [-0.22770166  0.47143584 -0.55156696 -0.24672067  0.1727817\n",
      "    0.68841785]\n",
      "  [-0.73660463  0.04830807  0.26371783  0.11195582  0.6287971\n",
      "   -0.6293106 ]]], shape=(1, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rel_pos_bias = RelativePositionBias(d_model=6, \n",
    "                                    num_buckets=5, max_relative_pos=5, \n",
    "                                    bidirectional=True)\n",
    "\n",
    "values = rel_pos_bias(query, key, log_bucket=False)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "       [-1,  0,  1,  2,  3,  4,  5],\n",
       "       [-2, -1,  0,  1,  2,  3,  4],\n",
       "       [-3, -2, -1,  0,  1,  2,  3],\n",
       "       [-4, -3, -2, -1,  0,  1,  2],\n",
       "       [-5, -4, -3, -2, -1,  0,  1],\n",
       "       [-6, -5, -4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_position = 5\n",
    "bucket_size = 5\n",
    "\n",
    "context_position = tf.range(qlen)[:, None]\n",
    "memory_position = tf.range(klen)[None, :]\n",
    "relative_pos = memory_position - context_position\n",
    "relative_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[5, 6, 7, 8, 9, 9, 9],\n",
       "       [4, 5, 6, 7, 8, 9, 9],\n",
       "       [3, 4, 5, 6, 7, 8, 9],\n",
       "       [2, 3, 4, 5, 6, 7, 8],\n",
       "       [1, 2, 3, 4, 5, 6, 7],\n",
       "       [1, 1, 2, 3, 4, 5, 6],\n",
       "       [1, 1, 1, 2, 3, 4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_pos = tf.clip_by_value(relative_pos, -max_position + 1, max_position - 1) + max_position\n",
    "relative_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([-4., -3., -3., -2., -1.,  0.,  1.,  2.,  3.,  3.], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_pos = tf.range(-max_position, max_position, dtype=tf.float32)\n",
    "sign = tf.sign(relative_pos)\n",
    "mid = bucket_size // 2\n",
    "abs_pos = tf.where((relative_pos < mid) & (relative_pos > -mid), mid - 1, tf.abs(relative_pos))\n",
    "log_pos = tf.math.ceil(tf.math.log(abs_pos / mid) / tf.math.log((max_position - 1) / mid) * (mid - 1)) + mid\n",
    "bucket_dict = tf.where(abs_pos <= mid, relative_pos, log_pos * sign)\n",
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([-4., -3., -3., -2., -1.,  0.,  1.,  2.,  3.,  3.], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(relative_pos.shape.rank - 1):\n",
    "    bucket_dict = tf.expand_dims(bucket_dict, 0)\n",
    "    bucket_dict = tf.gather(bucket_dict, indices=tf.cast(relative_pos, tf.int32), axis=-1)\n",
    "\n",
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 13:05:43.578076: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Index out of range using input dim 1; input has only 1 dims\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m bucket_dict \u001b[39m=\u001b[39m bucket_dict[:qlen, :]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m bucket_dict \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(bucket_dict, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeephub/home/users/dmoreno2016/ASTROMER/astromer_pe/astromer/presentation/experiments/astromer_1_pe/notebooks/positional_embeddings.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m bucket_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "bucket_dict = bucket_dict[:qlen, :]\n",
    "bucket_dict = tf.expand_dims(bucket_dict, axis=0)\n",
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 7), dtype=int32, numpy=\n",
       "array([[[ 0, -1, -2, -3, -4, -5, -6],\n",
       "        [ 1,  0, -1, -2, -3, -4, -5],\n",
       "        [ 2,  1,  0, -1, -2, -3, -4],\n",
       "        [ 3,  2,  1,  0, -1, -2, -3],\n",
       "        [ 4,  3,  2,  1,  0, -1, -2],\n",
       "        [ 5,  4,  3,  2,  1,  0, -1],\n",
       "        [ 6,  5,  4,  3,  2,  1,  0]]], dtype=int32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_ids = tf.range(qlen, dtype=tf.int32)\n",
    "k_ids = tf.range(klen, dtype=tf.int32)\n",
    "rel_pos_ids = q_ids[:, None] - tf.tile(tf.reshape(k_ids, [1, -1]), [qlen, 1])\n",
    "rel_pos_ids = rel_pos_ids[:qlen, :]\n",
    "rel_pos_ids = tf.expand_dims(rel_pos_ids, axis=0)\n",
    "rel_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=5>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_span = tf.cast(\n",
    "            tf.minimum(\n",
    "                tf.maximum(shape_list(query)[-2], shape_list(key)[-2]), max_position\n",
    "            ),\n",
    "            tf.int64,\n",
    "        )\n",
    "\n",
    "att_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_position - att_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=10>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_position + att_span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePosition(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, max_relative_position):\n",
    "        super(RelativePosition, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.max_relative_position = max_relative_position\n",
    "        self.embeddings_table = torch.nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n",
    "        torch.nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "    def forward(self, length_q, length_k):\n",
    "        range_vec_q = torch.arange(length_q)\n",
    "        range_vec_k = torch.arange(length_k)\n",
    "        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "        print(f\"distance_mat_clipped: {distance_mat_clipped.shape}\")\n",
    "        final_mat = distance_mat_clipped + self.max_relative_position\n",
    "        final_mat = torch.LongTensor(final_mat).cuda()\n",
    "        print(f\"final_mat: {final_mat.shape}\")\n",
    "        print(f\"final_mat \\n{final_mat}\")\n",
    "        embeddings = self.embeddings_table[final_mat].cuda()\n",
    "    \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.max_relative_position = 2\n",
    "\n",
    "        self.relative_position_k = RelativePosition(self.head_dim, self.max_relative_position)\n",
    "        self.relative_position_v = RelativePosition(self.head_dim, self.max_relative_position)\n",
    "\n",
    "        self.fc_q = torch.nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = torch.nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = torch.nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = torch.nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        len_k = key.shape[1]    #key = [batch_size, key_len, hid_dim]\n",
    "        len_q = query.shape[1]  #query = [batch_size, query_len, hid_dim]\n",
    "        len_v = value.shape[1]  #value = [batch_size, value_len, hid_dim]\n",
    "\n",
    "        # Create Q, K, and V using input vectors\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        # Compute Attention scores\n",
    "        r_q1 = query.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        r_k1 = key.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2)) \n",
    "\n",
    "        r_q2 = query.permute(1, 0, 2).contiguous().view(len_q, batch_size*self.n_heads, self.head_dim)\n",
    "        r_k2 = self.relative_position_k(len_q, len_k)\n",
    "        attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)\n",
    "        attn2 = attn2.contiguous().view(batch_size, self.n_heads, len_q, len_k)\n",
    "        attn = (attn1 + attn2) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        # Convert attention scores into probability distributions\n",
    "        attn = self.dropout(torch.softmax(attn, dim=-1))\n",
    "\n",
    "        #attn = [batch size, n heads, query len, key len]\n",
    "        r_v1 = value.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        weight1 = torch.matmul(attn, r_v1)\n",
    "        r_v2 = self.relative_position_v(len_q, len_v)\n",
    "        weight2 = attn.permute(2, 0, 1, 3).contiguous().view(len_q, batch_size*self.n_heads, len_k)\n",
    "        weight2 = torch.matmul(weight2, r_v2)\n",
    "        weight2 = weight2.transpose(0, 1).contiguous().view(batch_size, self.n_heads, len_q, self.head_dim)\n",
    "\n",
    "        x = weight1 + weight2\n",
    "        \n",
    "        # x = [batch size, n heads, query len, head dim]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        # x = [batch size, query len, n heads, head dim]\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        # x = [batch size, query len, hid dim]\n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        # x = [batch size, query len, hid dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "def shape_list(tensor: Union[tf.Tensor, np.ndarray]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Deal with dynamic shape in tensorflow cleanly.\n",
    "\n",
    "    Args:\n",
    "        tensor (`tf.Tensor` or `np.ndarray`): The tensor we want the shape of.\n",
    "\n",
    "    Returns:\n",
    "        `List[int]`: The shape of the tensor as a list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        return list(tensor.shape)\n",
    "\n",
    "    dynamic = tf.shape(tensor)\n",
    "\n",
    "    if tensor.shape == tf.TensorShape(None):\n",
    "        return dynamic\n",
    "\n",
    "    static = tensor.shape.as_list()\n",
    "\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "\n",
    "class RelativePositionBias(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_buckets, max_relative_pos, bidirectional, **kwargs):\n",
    "        super(RelativePositionBias, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_relative_pos = max_relative_pos\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.rel_embeddings = self.add_weight(shape=(self.num_buckets, self.d_model),\n",
    "                                             #initializer=self.init_freq,\n",
    "                                             trainable=True,\n",
    "                                             name='rel_emb')\n",
    "\n",
    "\n",
    "    def call(self, q, k, log_bucket=True):\n",
    "        batch_size, qlen = get_shape_list(q)[:2] \n",
    "        klen = get_shape_list(k)[1]\n",
    "        relative_pos = self.get_rel_pos(qlen, klen) # 200 x 200\n",
    "        if log_bucket:\n",
    "            relative_pos = self.make_log_bucket_position(relative_pos)\n",
    "        relative_pos = relative_pos[:qlen, :]\n",
    "        relative_pos = tf.expand_dims(relative_pos, axis=0)\n",
    "    \n",
    "        print('relative_pos:', relative_pos)\n",
    "        \n",
    "        shape_list_pos = shape_list(relative_pos)\n",
    "        print('shape_list_pos:', shape_list_pos)\n",
    "\n",
    "        if len(shape_list_pos) == 2:\n",
    "            relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)\n",
    "        elif len(shape_list_pos) == 3:\n",
    "            relative_pos = tf.expand_dims(relative_pos, 1)\n",
    "        # bsz x height x query x key\n",
    "        elif len(shape_list_pos) != 4:\n",
    "            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n",
    "\n",
    "        print('relative_pos:', relative_pos)\n",
    "\n",
    "        att_span = self.max_relative_pos\n",
    "        print('1. self.rel_embeddings:', self.rel_embeddings)\n",
    "        \n",
    "        self.rel_embeddings = tf.expand_dims(\n",
    "            self.rel_embeddings[self.max_relative_pos - att_span : self.max_relative_pos + att_span, :], 0\n",
    "        )\n",
    "\n",
    "        print('2. self.rel_embeddings:', self.rel_embeddings)\n",
    "\n",
    "    def get_rel_pos(self, qlen, klen):\n",
    "        context_position = tf.range(qlen)[:, None]\n",
    "        memory_position = tf.range(klen)[None, :]\n",
    "        relative_pos = memory_position - context_position\n",
    "        return relative_pos\n",
    "\n",
    "\n",
    "    def make_log_bucket_position(self, relative_pos):\n",
    "        sign = tf.math.sign(relative_pos)\n",
    "        mid = self.num_buckets // 2\n",
    "        abs_pos = tf.where((relative_pos < mid) & (relative_pos > -mid), mid - 1, tf.math.abs(relative_pos))\n",
    "        log_pos = (\n",
    "            tf.math.ceil(\n",
    "                tf.cast(tf.math.log(abs_pos / mid), tf.float32) / tf.math.log((self.max_relative_pos - 1) / mid) * (mid - 1)\n",
    "            )\n",
    "            + mid\n",
    "        )\n",
    "        bucket_pos = tf.cast(\n",
    "            tf.where(abs_pos <= mid, tf.cast(relative_pos, tf.float32), log_pos * tf.cast(sign, tf.float32)), tf.int32\n",
    "        )\n",
    "        return bucket_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None, return_mask=False):\n",
    "\n",
    "\tmatmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "\t# scale matmul_qk\n",
    "\tdk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "\tscaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "\tif mask is not None:\n",
    "\t\tprint('Using mask...')\n",
    "\t\tsteps = tf.shape(scaled_attention_logits)[2]\n",
    "\t\tmask_rshp = tf.tile(mask, [1,1,steps])\n",
    "\t\tmask_rshp += tf.transpose(mask_rshp, [0,2,1])\n",
    "\t\tmask_rshp = tf.minimum(1., mask_rshp)\n",
    "\t\tmask_rshp = tf.expand_dims(mask_rshp, 1)\n",
    "\t\tscaled_attention_logits += (mask_rshp*-1e9)\n",
    "\n",
    "\t# softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "\tattention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1, name='MaskedSoftMax')  # (..., seq_len_q, seq_len_k)\n",
    "\toutput = tf.matmul(attention_weights, v, name='Z')  # (..., seq_len_q, depth_v)\n",
    "\tif return_mask:\n",
    "\t\treturn output, attention_weights, mask_rshp\n",
    "\treturn output, attention_weights\n",
    "\n",
    "\n",
    "class HeadAttentionMulti(tf.keras.layers.Layer):\n",
    "\tdef __init__(self, head_dim, num_heads):\n",
    "\t\tsuper(HeadAttentionMulti, self).__init__()\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.head_dim = head_dim\n",
    "\t\t\n",
    "\t\tself.d_model = self.num_heads * self.head_dim\n",
    "\t\tself.depth = self.d_model // self.num_heads # final dimension\n",
    "\t\t\n",
    "\t\tself.wq = tf.keras.layers.Dense(self.d_model, name='WQ')\n",
    "\t\tself.wk = tf.keras.layers.Dense(self.d_model, name='WK')\n",
    "\t\tself.wv = tf.keras.layers.Dense(self.d_model, name='WV')\n",
    "\t\tself.dense = tf.keras.layers.Dense(self.d_model, name='attmerge')\n",
    "\n",
    "\t\t#self.scaled_dot_product_attention = {\n",
    "\t\t#\t'normal': scaled_dot_product_attention\n",
    "\t\t#}\n",
    "\n",
    "\tdef split_heads(self, x, batch_size, name='qkv'):\n",
    "\t\t\"\"\"Split the last dimension into (num_heads, depth).\n",
    "\t\tTranspose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "\t\t\"\"\"\n",
    "\t\tx = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "\t\treturn tf.transpose(x, perm=[0, 2, 1, 3], name=name)\n",
    "\n",
    "\tdef call(self, x, training, mask=None):\n",
    "\t\tbatch_size = tf.shape(x)[0]\n",
    "\n",
    "\t\tq = self.wq(x)  # (batch_size, seq_len, d_model)\n",
    "\t\tk = self.wk(x)  # (batch_size, seq_len, d_model)\n",
    "\t\tv = self.wv(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "\t\tq = self.split_heads(q, batch_size, name='Q')  # (batch_size, num_heads, seq_len_q, depth)\n",
    "\t\tk = self.split_heads(k, batch_size, name='K')  # (batch_size, num_heads, seq_len_k, depth)\n",
    "\t\tv = self.split_heads(v, batch_size, name='V')  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "\t\t# scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "\t\t# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\t\tscaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "\t\tscaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "\t\tconcat_attention = tf.reshape(scaled_attention,\n",
    "\t\t\t\t\t\t\t\t\t\t(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "\t\toutput = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\t\t\n",
    "\t\treturn output, attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
