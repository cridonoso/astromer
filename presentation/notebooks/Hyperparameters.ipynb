{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for the best hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import optuna\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization, \\\n",
    "                                    Dense, \\\n",
    "                                    LSTM, \\\n",
    "                                    LayerNormalization, \\\n",
    "                                    Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(source):\n",
    "    file = open(source, 'rb')\n",
    "    hf = h5py.File(file)\n",
    "    att = hf['att'][()]\n",
    "    x = hf['x'][()]\n",
    "    t = hf['t'][()]\n",
    "    lc = np.concatenate([t, x], 2)\n",
    "\n",
    "    y = hf['y'][()]\n",
    "    l = hf['id'][()]\n",
    "    m = 1. - hf['m'][()]\n",
    "    return att, y, l, m, lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, y_0, l_0, m_0, lc_0 = load_embeddings('../../embeddings/alcock/train.h5')\n",
    "\n",
    "n_classes = len(np.unique(y_0))\n",
    "x_train = np.sum(x_0*m_0, 1)/tf.reduce_sum(m_0, 1)\n",
    "y_train = tf.one_hot(y_0, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_objetive(trial):\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "            \n",
    "    inputs = tf.keras.Input(shape=(256))\n",
    "    x_mean = tf.expand_dims(tf.reduce_mean(inputs, 1), 1)\n",
    "    x_std = tf.expand_dims(tf.math.reduce_std(inputs, 1), 1)\n",
    "    \n",
    "    x = (inputs - x_mean)/x_std\n",
    "    for i in range(n_layers):\n",
    "        num_hidden = trial.suggest_int(f'n_units_l{i}', 4, 128, log=True)\n",
    "        x = Dense(num_hidden, activation='relu')(x)\n",
    "        \n",
    "    x = Dense(n_classes)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "           \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(trial):\n",
    "    # We optimize the choice of optimizers as well as their parameters.\n",
    "    kwargs = {}\n",
    "    optimizer_options = [\"RMSprop\", \"Adam\", \"SGD\"]\n",
    "    optimizer_selected = trial.suggest_categorical(\"optimizer\", optimizer_options)\n",
    "    if optimizer_selected == \"RMSprop\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\n",
    "            \"rmsprop_learning_rate\", 1e-5, 1e-1, log=True\n",
    "        )\n",
    "        kwargs[\"decay\"] = trial.suggest_float(\"rmsprop_decay\", 0.85, 0.99)\n",
    "        kwargs[\"momentum\"] = trial.suggest_float(\"rmsprop_momentum\", 1e-5, 1e-1, log=True)\n",
    "    elif optimizer_selected == \"Adam\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    elif optimizer_selected == \"SGD\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\n",
    "            \"sgd_opt_learning_rate\", 1e-5, 1e-1, log=True\n",
    "        )\n",
    "        kwargs[\"momentum\"] = trial.suggest_float(\"sgd_opt_momentum\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, optimizer, dataset, mode=\"eval\"):\n",
    "    accuracy = tf.metrics.Accuracy(\"accuracy\", dtype=tf.float32)\n",
    "\n",
    "    for batch, (images, labels) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images, training=(mode == \"train\"))\n",
    "            loss_value = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "            )\n",
    "            if mode == \"eval\":\n",
    "                accuracy(\n",
    "                    tf.argmax(logits, axis=1, output_type=tf.int64), tf.cast(labels, tf.int64)\n",
    "                )\n",
    "            else:\n",
    "                grads = tape.gradient(loss_value, model.variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Get MNIST data.\n",
    "    train_ds, valid_ds = get_mnist()\n",
    "\n",
    "    # Build model and optimizer.\n",
    "    model = create_model(trial)\n",
    "    optimizer = create_optimizer(trial)\n",
    "\n",
    "    # Training and validating cycle.\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        for _ in range(EPOCHS):\n",
    "            learn(model, optimizer, train_ds, \"train\")\n",
    "\n",
    "        accuracy = learn(model, optimizer, valid_ds, \"eval\")\n",
    "\n",
    "    # Return last validation accuracy.\n",
    "    return accuracy.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
