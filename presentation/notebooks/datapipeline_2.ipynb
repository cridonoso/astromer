{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f82b6f2",
   "metadata": {},
   "source": [
    "# Creating records \n",
    "## Pipeline 2.0\n",
    "##### ASTROMER dev team\n",
    "\n",
    "*July 07 2023*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135c8403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n"
     ]
    }
   ],
   "source": [
    "cd /home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb0e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.data.record import DataPipeline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dd79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "METAPATH = './data/raw_data_parquet/alcock/metadata.parquet'\n",
    "OBSPATH  = './data/raw_data_parquet/alcock/light_curves/'\n",
    "config_path = './data/raw_data_parquet/alcock/config.toml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff57534",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_parquet(METAPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dfb7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['Class'] = pd.Categorical(metadata['Class'])\n",
    "metadata['Label'] = metadata['Class'].cat.codes\n",
    "metadata['Path'] = metadata['Path'].apply(lambda x: os.path.join(OBSPATH, x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234d8678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Path</th>\n",
       "      <th>Band</th>\n",
       "      <th>sset</th>\n",
       "      <th>newID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78.5859.18</td>\n",
       "      <td>Cep_0</td>\n",
       "      <td>./data/raw_data_parquet/alcock/light_curves/78...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>15693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Class                                               Path  Band  \\\n",
       "9  78.5859.18  Cep_0  ./data/raw_data_parquet/alcock/light_curves/78...   1.0   \n",
       "\n",
       "    sset  newID  Label  \n",
       "9  train  15693      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039028e",
   "metadata": {},
   "source": [
    "### Using DataPipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a39da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 20 samples loaded\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of DataPipeline\n",
    "pipeline = DataPipeline(metadata=metadata,\n",
    "                        config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4df0757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['float', 'float', 'float']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.sequential_features_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064aad8b",
   "metadata": {},
   "source": [
    "To create training, validation, and testing splits we need to use the `train_val_test` method \n",
    "```\n",
    "train_val_test(val_frac=0.2,\n",
    "               test_frac=0.2,\n",
    "               test_meta=None,\n",
    "               val_meta=None,\n",
    "               shuffle=True,\n",
    "               id_column_name=None,\n",
    "               k_fold=1)\n",
    "``` \n",
    "where `val_frac` and `test_frac` are percentages containing the fraction of the metadata to be used as validation and testing subset respectively. \n",
    "\n",
    "Additionally, you can use `val_meta` and `test_meta` to use a preselected subset. **Notice that if you employ your own test/val subset, you should match one of the identifier columns of the main DataFrame** (by default it will assume the first column of the dataset is the identifier). \n",
    "\n",
    "Both `test_meta` and `val_meta` must be list of `DataFrames`\n",
    "\n",
    "For cross-validation purposes, we can also sample different folds from the same dataset by using the `train_val_test(..., k_fold=1)` parameter.\n",
    "\n",
    "If $k>1$ and **you want to use a predefined test/val selection**, you should pass a list of `DataFrame`s associated with each `test_meta`/`val_meta` fold as appropriate.\n",
    "\n",
    "Don't worry about removing duplicated indices, the `train_val_test` method will do it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "918c742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_metadata = metadata.sample(n=100)\n",
    "test_metadata = metadata[metadata['sset'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f193ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using ID col as sample identifier\n",
      "[INFO] Shuffling\n",
      "[INFO] Shuffling\n",
      "[INFO] Shuffling\n"
     ]
    }
   ],
   "source": [
    "k_folds = 3\n",
    "pipeline.train_val_test(val_frac=0.2, \n",
    "                        test_meta=None, \n",
    "                        k_fold=k_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013795b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do 3-folds partitions have the same elements:  False\n",
      "Do 3-folds partitions have the same elements:  False\n"
     ]
    }
   ],
   "source": [
    "a = pipeline.metadata['subset_0']\n",
    "for k in range(k_folds):\n",
    "    if k == 0: continue\n",
    "    b = pipeline.metadata[f'subset_{k}']\n",
    "    c = np.array_equal(a[a != 'test'].values, b[b!= 'test'].values)\n",
    "    a = b\n",
    "    print('Do {}-folds partitions have the same elements: '.format(k_folds), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bcf6f3",
   "metadata": {},
   "source": [
    "Now our metadata will contain an extra-column `subset` for the corresponding subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f631c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Path</th>\n",
       "      <th>Band</th>\n",
       "      <th>sset</th>\n",
       "      <th>newID</th>\n",
       "      <th>Label</th>\n",
       "      <th>subset_0</th>\n",
       "      <th>subset_1</th>\n",
       "      <th>subset_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23.3062.469</td>\n",
       "      <td>RRc</td>\n",
       "      <td>./data/raw_data_parquet/alcock/light_curves/23...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>8121</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>test</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82.8408.718</td>\n",
       "      <td>RRab</td>\n",
       "      <td>./data/raw_data_parquet/alcock/light_curves/82...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.3560.324</td>\n",
       "      <td>RRc</td>\n",
       "      <td>./data/raw_data_parquet/alcock/light_curves/10...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>961</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78.5859.18</td>\n",
       "      <td>Cep_0</td>\n",
       "      <td>./data/raw_data_parquet/alcock/light_curves/78...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>15693</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Class                                               Path  \\\n",
       "13  23.3062.469    RRc  ./data/raw_data_parquet/alcock/light_curves/23...   \n",
       "2   82.8408.718   RRab  ./data/raw_data_parquet/alcock/light_curves/82...   \n",
       "18  10.3560.324    RRc  ./data/raw_data_parquet/alcock/light_curves/10...   \n",
       "9    78.5859.18  Cep_0  ./data/raw_data_parquet/alcock/light_curves/78...   \n",
       "\n",
       "    Band   sset  newID  Label subset_0 subset_1    subset_2  \n",
       "13   1.0  train   8121      4     test     test       train  \n",
       "2    1.0  train  20001      3    train    train       train  \n",
       "18   1.0  train    961      4     test    train       train  \n",
       "9    1.0  train  15693      0    train    train  validation  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.metadata.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6e88a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 10) (3, 10) (4, 10)\n",
      "test in train?:  False \n",
      " val in train?:  False \n",
      " val in test?:  False\n",
      "(13, 10) (3, 10) (4, 10)\n",
      "test in train?:  False \n",
      " val in train?:  False \n",
      " val in test?:  False\n",
      "(13, 10) (3, 10) (4, 10)\n",
      "test in train?:  False \n",
      " val in train?:  False \n",
      " val in test?:  False\n"
     ]
    }
   ],
   "source": [
    "for k in range(k_folds):\n",
    "    train_subset = pipeline.metadata[pipeline.metadata[f'subset_{k}'] == 'train']\n",
    "    val_subset   = pipeline.metadata[pipeline.metadata[f'subset_{k}'] == 'validation']\n",
    "    test_subset  = pipeline.metadata[pipeline.metadata[f'subset_{k}'] == 'test']\n",
    "\n",
    "    print(train_subset.shape, val_subset.shape, test_subset.shape)\n",
    "\n",
    "    print('test in train?: ', test_subset['ID'].isin(train_subset['ID']).all(),'\\n',\n",
    "          'val in train?: ', val_subset['ID'].isin(train_subset['ID']).all(),'\\n',\n",
    "          'val in test?: ', val_subset['ID'].isin(test_subset['ID']).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e25b1f",
   "metadata": {},
   "source": [
    "Notice if you want to redo, you must initialize the object `DataPipeline` again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377c4fe",
   "metadata": {},
   "source": [
    "Now it is **time to the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "539e35af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ID', 'Label', 'Class', 'Band'], ['string', 'integer', 'string', 'integer'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.context_features, pipeline.context_features_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "923c9a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 20:44:09,629 - INFO - Starting DataPipeline operations\n",
      "\n",
      "  0%|\u001b[38;2;0;255;0m                                                                                                                                              \u001b[0m| 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/home/src/data/record.py:383\u001b[0m, in \u001b[0;36mDataPipeline.run\u001b[0;34m(self, observations_path, metadata_path, n_jobs, elements_per_shard)\u001b[0m\n\u001b[1;32m    380\u001b[0m fold_groups \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubset\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[1;32m    381\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(fold_groups, colour\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#00ff00\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# progress bar\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m new_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_all_parquets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_df \u001b[38;5;241m=\u001b[39m new_df\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_n, fold_col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n",
      "File \u001b[0;32m/home/src/data/record.py:325\u001b[0m, in \u001b[0;36mDataPipeline.read_all_parquets\u001b[0;34m(self, observations_path, metadata_path)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Read the parquet filez lazily\u001b[39;00m\n\u001b[1;32m    324\u001b[0m paths \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(observations_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 325\u001b[0m scan \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Using partial information, extract only the necessary objects\u001b[39;00m\n\u001b[1;32m    328\u001b[0m ID_series \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_column]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/polars/utils/deprecation.py:136\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    133\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    134\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[1;32m    135\u001b[0m     )\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/polars/utils/deprecation.py:136\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    133\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    134\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[1;32m    135\u001b[0m     )\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/polars/io/parquet/functions.py:311\u001b[0m, in \u001b[0;36mscan_parquet\u001b[0;34m(source, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, rechunk, low_memory, cache, storage_options, retries)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     source \u001b[38;5;241m=\u001b[39m [normalize_filepath(source) \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m source]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLazyFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scan_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhive_partitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhive_partitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/polars/lazyframe/frame.py:466\u001b[0m, in \u001b[0;36mLazyFrame._scan_parquet\u001b[0;34m(cls, source, n_rows, cache, parallel, rechunk, row_index_name, row_index_offset, storage_options, low_memory, use_statistics, hive_partitioning, retries)\u001b[0m\n\u001b[1;32m    463\u001b[0m     storage_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ldf \u001b[38;5;241m=\u001b[39m \u001b[43mPyLazyFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_from_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_prepare_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloud_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhive_partitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhive_partitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "var = pipeline.run(observations_path=OBSPATH, \n",
    "                   metadata_path=METAPATH,\n",
    "                   n_jobs=8,\n",
    "                   elements_per_shard=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b92d9",
   "metadata": {},
   "source": [
    "### Customize what happens within the preprocess function\n",
    "### (NOT WORKING YET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1231e7",
   "metadata": {},
   "source": [
    "You must keep the same parameters of the method i.e., `row, context_features, sequential_features`. \n",
    "\n",
    "Also the **output** should be tuple containing the lightcurve (`pd.DataFrame`) and the context values (`dict`)\n",
    "\n",
    "\n",
    "To modify the `process_sample` method we need to create a new class (`MyPipeline`) that inherits from `DataPipeline` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f73c94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "class MyPipeline(DataPipeline):\n",
    "    def lightcurve_step(self, inputs):\n",
    "        \"\"\"\n",
    "        Preprocessing applied to each light curve separately\n",
    "        \"\"\"\n",
    "        # First feature is time\n",
    "        inputs = inputs.sort(self.sequential_features[0], descending=True) \n",
    "        print(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def observations_step(self):\n",
    "        \"\"\"\n",
    "        Preprocessing applied to all observations. Filter only\n",
    "        \"\"\"\n",
    "        fn_0 = pl.col(\"red error\") <1.  # Clean the data on the big lazy dataframe\n",
    "        fn_1 = pl.col(\"red magnitude\") <2.\n",
    "        return fn_0 & fn_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534f698",
   "metadata": {},
   "source": [
    "Next steps are the same as using the original `DataPipeline` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3fb0e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1529386 samples loaded\n"
     ]
    }
   ],
   "source": [
    "custom_pipeline = MyPipeline(metadata=metadata,\n",
    "                             config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "859de195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using ID col as sample identifier\n",
      "[INFO] Shuffling\n",
      "[INFO] Shuffling\n",
      "[INFO] Shuffling\n"
     ]
    }
   ],
   "source": [
    "test_metadata = metadata.sample(n=100)\n",
    "k_folds = 3\n",
    "custom_pipeline.train_val_test(val_frac=0.2, \n",
    "                               test_meta=[test_metadata]*k_folds, \n",
    "                               k_fold=k_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b9322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 20:45:11,747 - INFO - Starting DataPipeline operations\n",
      "\n",
      "\n",
      "\n",
      "  0%|\u001b[38;2;0;255;0m                                                                                                                                              \u001b[0m| 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "var = custom_pipeline.run(observations_path=OBSPATH, \n",
    "                           metadata_path=METAPATH,\n",
    "                           n_jobs=8,\n",
    "                           elements_per_shard=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef30a7",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18cb81f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from src.data.record import deserialize\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "622f6480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'id_column': {'value': 'newID', 'dtype': 'integer'}, 'target': {'value': './data/records_parquet/', 'dtype': 'string'}}, 'context_features': {'value': ['ID', 'Label', 'Class', 'Band'], 'dtypes': ['string', 'integer', 'string', 'integer']}, 'sequential_features': {'value': ['mjd', 'mag', 'errmag'], 'dtypes': ['float', 'float', 'float']}}\n"
     ]
    }
   ],
   "source": [
    "root = './data/records_parquet/fold_0/test/'\n",
    "record_files = glob.glob(os.path.join(root, '*.record'))\n",
    "raw_dataset = tf.data.TFRecordDataset(record_files)\n",
    "raw_dataset = raw_dataset.map(lambda x: deserialize(x, root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76646211",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in raw_dataset.take(1):\n",
    "    print(x.keys())\n",
    "    plt.plot(x['input'][0, ..., 0], x['input'][0, ..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f201252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772cd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
