{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/astromer/presentation/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/astromer\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from src.data.record import DataPipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import polars as pl\n",
    "import os\n",
    "from glob import glob\n",
    "import sys\n",
    "# sys.path.append('/home/Samsung2TB/')\n",
    "# sys.path.append('/home/Samsung2TB/astromer/')\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from src.data.record import DataPipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from time import time\n",
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local path\n",
    "local_path = '/home/ubuntu/astromer/data/raw_data/alcock/'\n",
    "\n",
    "# Define Metadata and LCDIR paths\n",
    "METAPATH = join(local_path, 'new_metadata.parquet')\n",
    "LCDIR =  join(local_path, 'parquets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Path</th>\n",
       "      <th>Band</th>\n",
       "      <th>newID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3319.10</td>\n",
       "      <td>LPV</td>\n",
       "      <td>1.3319.10.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3441.15</td>\n",
       "      <td>Cep_0</td>\n",
       "      <td>1.3441.15.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3441.25</td>\n",
       "      <td>LPV</td>\n",
       "      <td>1.3441.25.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3441.45</td>\n",
       "      <td>Cep_0</td>\n",
       "      <td>1.3441.45.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3441.1031</td>\n",
       "      <td>RRab</td>\n",
       "      <td>1.3441.1031.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21439</th>\n",
       "      <td>9.5608.870</td>\n",
       "      <td>RRab</td>\n",
       "      <td>9.5608.870.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21439</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21440</th>\n",
       "      <td>9.5608.946</td>\n",
       "      <td>RRab</td>\n",
       "      <td>9.5608.946.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21440</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21441</th>\n",
       "      <td>9.5609.22</td>\n",
       "      <td>EC</td>\n",
       "      <td>9.5609.22.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21441</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21442</th>\n",
       "      <td>9.5609.790</td>\n",
       "      <td>RRab</td>\n",
       "      <td>9.5609.790.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21442</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21443</th>\n",
       "      <td>9.5609.798</td>\n",
       "      <td>RRab</td>\n",
       "      <td>9.5609.798.dat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21443</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21444 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Class             Path  Band  newID  Label\n",
       "0        1.3319.10    LPV    1.3319.10.dat   1.0      0      3\n",
       "1        1.3441.15  Cep_0    1.3441.15.dat   1.0      1      0\n",
       "2        1.3441.25    LPV    1.3441.25.dat   1.0      2      3\n",
       "3        1.3441.45  Cep_0    1.3441.45.dat   1.0      3      0\n",
       "4      1.3441.1031   RRab  1.3441.1031.dat   1.0      4      4\n",
       "...            ...    ...              ...   ...    ...    ...\n",
       "21439   9.5608.870   RRab   9.5608.870.dat   1.0  21439      4\n",
       "21440   9.5608.946   RRab   9.5608.946.dat   1.0  21440      4\n",
       "21441    9.5609.22     EC    9.5609.22.dat   1.0  21441      2\n",
       "21442   9.5609.790   RRab   9.5609.790.dat   1.0  21442      4\n",
       "21443   9.5609.798   RRab   9.5609.798.dat   1.0  21443      4\n",
       "\n",
       "[21444 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read metadata\n",
    "metadata = pd.read_parquet(METAPATH)\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process metadata\n",
    "metadata['Class'] = pd.Categorical(metadata['Class'])\n",
    "metadata['Label'] = metadata['Class'].cat.codes\n",
    "metadata['Path'] = metadata['Path'].apply(lambda x: os.path.join(LCDIR, x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the metadata\n",
    "metadata_sample = metadata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Path</th>\n",
       "      <th>Band</th>\n",
       "      <th>newID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3319.10</td>\n",
       "      <td>LPV</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3441.15</td>\n",
       "      <td>Cep_0</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3441.25</td>\n",
       "      <td>LPV</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3441.45</td>\n",
       "      <td>Cep_0</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3441.1031</td>\n",
       "      <td>RRab</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21439</th>\n",
       "      <td>9.5608.870</td>\n",
       "      <td>RRab</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21439</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21440</th>\n",
       "      <td>9.5608.946</td>\n",
       "      <td>RRab</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21440</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21441</th>\n",
       "      <td>9.5609.22</td>\n",
       "      <td>EC</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21441</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21442</th>\n",
       "      <td>9.5609.790</td>\n",
       "      <td>RRab</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21442</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21443</th>\n",
       "      <td>9.5609.798</td>\n",
       "      <td>RRab</td>\n",
       "      <td>/home/ubuntu/astromer/data/raw_data/alcock/par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21443</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21444 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Class                                               Path  \\\n",
       "0        1.3319.10    LPV  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "1        1.3441.15  Cep_0  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "2        1.3441.25    LPV  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "3        1.3441.45  Cep_0  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "4      1.3441.1031   RRab  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "...            ...    ...                                                ...   \n",
       "21439   9.5608.870   RRab  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "21440   9.5608.946   RRab  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "21441    9.5609.22     EC  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "21442   9.5609.790   RRab  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "21443   9.5609.798   RRab  /home/ubuntu/astromer/data/raw_data/alcock/par...   \n",
       "\n",
       "       Band  newID  Label  \n",
       "0       1.0      0      3  \n",
       "1       1.0      1      0  \n",
       "2       1.0      2      3  \n",
       "3       1.0      3      0  \n",
       "4       1.0      4      4  \n",
       "...     ...    ...    ...  \n",
       "21439   1.0  21439      4  \n",
       "21440   1.0  21440      4  \n",
       "21441   1.0  21441      2  \n",
       "21442   1.0  21442      4  \n",
       "21443   1.0  21443      4  \n",
       "\n",
       "[21444 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 21444 samples loaded\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of DataPipeline\n",
    "myPipeline = DataPipeline(metadata=metadata_sample, \n",
    "                            context_features=['ID', 'Label', 'Class'],\n",
    "                            sequential_features=['mjd', 'mag'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using ID col as sample identifier\n",
      "[INFO] Shuffling\n",
      "[INFO] Shuffling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare for k-fold cross-validation\n",
    "test_metadata = metadata_sample.sample(n=1000)\n",
    "k_folds = 2\n",
    "# Split the data into training, validation, and test sets\n",
    "myPipeline.train_val_test(val_frac=0.2, \n",
    "                        test_meta=[test_metadata], \n",
    "                        k_fold=k_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 12:32:00,949 - INFO - Starting the writing process for the config file.\n",
      "2023-06-21 12:32:00,951 - INFO - Config file written successfully to ./config.toml\n"
     ]
    }
   ],
   "source": [
    "# Save the configuration into a .toml file\n",
    "config_file = './config.toml'\n",
    "myPipeline.write_config(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]2023-06-21 12:32:26,236 - INFO - Reading parquet files\n",
      "2023-06-21 12:32:26,283 - INFO - Processing train subset_0\n",
      "Processing train subset_0:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]2023-06-21 12:32:26,293 - INFO - Writting train fold 0\n",
      "Writting train fold 0:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]    2023-06-21 12:32:26,295 - INFO - Processing test subset_0\n",
      "Processing test subset_0:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]2023-06-21 12:32:26,299 - INFO - Writting test fold 0\n",
      "Writting test fold 0:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]    2023-06-21 12:32:26,301 - INFO - Processing validation subset_0\n",
      "Processing validation subset_0:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]2023-06-21 12:32:26,305 - INFO - Writting validation fold 0\n",
      "Writting validation fold 0:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]    2023-06-21 12:32:26,308 - INFO - Processing train subset_1\n",
      "Processing train subset_1:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s] 2023-06-21 12:32:26,313 - INFO - Writting train fold 1\n",
      "Writting train fold 1:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]    2023-06-21 12:32:26,315 - INFO - Processing validation subset_1\n",
      "Processing validation subset_1:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]2023-06-21 12:32:26,319 - INFO - Writting validation fold 1\n",
      "Writting validation fold 1:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]    2023-06-21 12:32:26,321 - INFO - Processing test subset_1\n",
      "Processing test subset_1:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]  2023-06-21 12:32:26,325 - INFO - Writting test fold 1\n",
      "Writting test fold 1:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]    2023-06-21 12:32:26,326 - INFO - Finished execution of DataPipeline operations\n",
      "2023-06-21 12:32:26,327 - INFO - Starting the writing process for the config file.\n",
      "2023-06-21 12:32:26,328 - INFO - Config file written successfully to ./config.toml\n",
      "2023-06-21 12:32:26,328 - INFO - Config file written\n",
      "Writting test fold 1:   0%|\u001b[38;2;0;255;0m          \u001b[0m| 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09557199478149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create TFRecord files and measure time taken\n",
    "a = time()\n",
    "var = myPipeline.run(LCDIR, METAPATH, n_jobs=2,)\n",
    "b = time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid container provided to prepare_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25871/3492243670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mshards_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melements_per_shard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mshard_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshards_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mDataPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/astromer/src/data/record.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, container, elements_per_shard, subset, fold_n)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;34m\"\"\"Prepare the data to be saved as records\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontainer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__iter__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid container provided to prepare_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Number of objects in the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid container provided to prepare_data"
     ]
    }
   ],
   "source": [
    "for subset in ['train', 'val', 'test']:\n",
    "    for fold_n in range(k_folds):\n",
    "        shards_data, shard_paths = myPipeline.prepare_data(var, elements_per_shard=10, subset=subset, fold_n=fold_n)\n",
    "        for shard_data, shard_path in zip(shards_data, shard_paths):\n",
    "            DataPipeline.aux_serialize(shard_data, shard_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 12:33:04,070 - INFO - Starting to read the file from ./data.tfrecord.\n",
      "2023-06-21 12:33:04.117199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.276985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.277603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.278799: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 12:33:04.279314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.279905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.280480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.935319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.935929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.936529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 12:33:04.937067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13791 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "2023-06-21 12:33:05,222 - INFO - Successfully read the file from ./data.tfrecord.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's read back the records and process them\n",
    "record_file = './data.tfrecord'\n",
    "# Read the .record file\n",
    "raw_dataset = myPipeline.open_and_read_record(record_file)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "./data.tfrecord; No such file or directory [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25871/348655917.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Deserialize each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mraw_record\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    784\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2843\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2845\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2846\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./data.tfrecord; No such file or directory [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# Deserialize each sample\n",
    "processed_data = []\n",
    "for raw_record in raw_dataset:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    processed_data.append(myPipeline.deserialize(example.SerializeToString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2_p38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
